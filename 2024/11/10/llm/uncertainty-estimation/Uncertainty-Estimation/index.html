<!DOCTYPE html>
<html lang="zh-cn,en,default">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="崔若晨 Ruochen Cui">


  <meta name="subtitle" content="崔若晨">


  <meta name="description" content="电子科技大学四年级本科生

A fourth year undergraduate student at UESTC
">


  <meta name="keywords" content="Ruochen Cui,崔若晨,Artificial Intelligence">


<title>Uncertainty Estimation 论文粗读 | Ruochen Cui</title>



<link rel="icon" href="/web_ico.ico">


<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"
/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/css/search.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ic:round-dark-mode" : "ic:round-light-mode"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<meta name="generator" content="Hexo 7.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body 
  class="
    bg-[var(--c-0)]
    text-[var(--c-80)]
  ">
  <!-- The navigation bar -->
<header class="
    flex flex-row items-center
    w-full
    pr-4
    z-10
    border-b-[1px]
    border-b-[var(--c-border)]
    dark:bg-[var(--c-0)]
    dark:border-b-[var(--c-0)]
    gap-2
    h-[var(--h-header)]
    text-[var(--c-80)]
">
  <!-- Left part -->
  <div class="overflow-hidden h-full flex flex-row items-center">
    <!-- Site Title on the top left -->
    <a href="/" class="
            whitespace-nowrap
            text-2xl
            text-[var(--c-theme)]
            hover:text-[var(--c-theme)]
            pl-4
            font-black
            bg-gradient-to-r from-cyan-500
            to-blue-500 bg-clip-text text-transparent
          ">
      Ruochen Cui
    </a>
  </div>
  <!-- Div for pushing items to both sides -->
  <div class="flex-1"></div>
  <!-- Right part -->
  <div class="flex flex-row items-center z-20 h-full">
    <!-- Page links -->
    <div class="hidden sm:flex flex-row h-full">
      
      
      
      
      
      
      <a href="/./archives" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:inbox-fill" width="22">
        </iconify-icon>
        
        
        <p>Posts</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./publications" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:science-fill" width="22">
        </iconify-icon>
        
        
        <p>Publications</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./about" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:user-info-fill" width="22">
        </iconify-icon>
        
        
        <p>About</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./categories" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:classify-2-fill" width="22">
        </iconify-icon>
        
        
        <p>Categories</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./tags" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:tag-fill" width="22">
        </iconify-icon>
        
        
        <p>Tags</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./index" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:home-2-fill" width="22">
        </iconify-icon>
        
        
      </a>
      
    </div>
    <!-- Icons on the right -->
    <div class="flex flex-row items-center">

      <!-- TODO: Add search icon here -->

      <!-- Dark/light toggle icon -->
      <a class="flex group p-1" title="toggle theme" id="toggle-dark">
        <iconify-icon class="transition-transform
                    group-hover:rotate-[45deg]
                    group-hover:scale-125
                    group-hover:text-[var(--c-theme)]" width="24" id="theme-icon">
        </iconify-icon>
      </a>
      <!-- Icon for dropout menu on small screens -->
      <div class="flex p-1 mx-1 sm:hidden">
        <a class="w-5 h-5" aria-hidden="true" id="open-menu">
          <iconify-icon width="24" icon="mingcute:menu-fill" class="transition-transform hover:scale-125 hover:rotate-[5deg]">
          </iconify-icon>
        </a>
        <a class="w-5 h-5 hidden" aria-hidden="true" id="close-menu">
          <iconify-icon width="24" icon="mingcute:close-circle-fill" class="transition-transform hover:scale-125 hover:rotate-[80deg]">
          </iconify-icon>
        </a>
      </div>
    </div>
  </div>
</header>

<!-- Dropdown menu on small screens -->
<div id="menu-panel" class="
        h-0
        overflow-hidden
        sm:hidden
        w-full
        z-10
        rounded
    ">
  <div id="menu-content" class="
        flex
        flex-row
        justify-center
        items-center
        font-bold
        text-xl
        border-b-[1px]
        relative
        z-20
        border-[var(--c-sep)]
        px-2
        py-2
        -translate-y-full
        transition-transform
        duration-200
        ">
    
    
    
    <a href="/./archives" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:inbox-fill" width="22">
      </iconify-icon>
      <p>
        posts
      </p>
    </a>
    
    
    
    
    <a href="/./publications" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:science-fill" width="22">
      </iconify-icon>
      <p>
        publications
      </p>
    </a>
    
    
    
    
    <a href="/./about" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:user-info-fill" width="22">
      </iconify-icon>
      <p>
        about
      </p>
    </a>
    
    
    
    
    <a href="/./categories" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:classify-2-fill" width="22">
      </iconify-icon>
      <p>
        categories
      </p>
    </a>
    
    
    
    
    <a href="/./tags" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:tag-fill" width="22">
      </iconify-icon>
      <p>
        tags
      </p>
    </a>
    
    
    
    
    <a href="/./index" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:home-2-fill" width="22">
      </iconify-icon>
      <p>
        home
      </p>
    </a>
    
    
  </div>
</div>
  <main>
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">

  
<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

    <!-- toc -->
    
  <!-- tocbot -->
<nav class="post-toc toc text-sm w-40 relative top-32 right-4 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- Post header before content -->
  <header class="py-4">
    <div class="flex flex-col gap-2 pt-4 md:pt-6">
      <!-- Title -->
      <div id="article-title" class="leading-snug">
        <p class="text-3xl font-bold text-[var(--c-100)] mb-4">Uncertainty Estimation 论文粗读</p>
      </div>
      <!-- Meta data -->
      <div>
        <section class="
          flex flex-col gap-x-2 gap-y-1 text-sm text-[var(--c-100)]">
          <div class="flex flex-wrap items-center gap-x-2 gap-y-1">
            <!-- Dates -->
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:add-circle-fill" ></iconify-icon>
              Created: <time class="w-max">2024-11-10</time>
            </div>
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:refresh-3-fill" ></iconify-icon>
              Edited: <time class="w-max">2024-12-19</time>
            </div>
          </div>
          <div class="flex flex-wrap items-center gap-x-3 gap-y-3">
            <!-- Author -->
            
              <span class="flex items-center gap-1 group">
                <iconify-icon width="18" icon="mingcute:user-edit-fill" ></iconify-icon>
                <p>myself</p>
              </span>
            

            <!-- Word count -->
            <span class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:book-2-fill" ></iconify-icon>
              <span>151 words, 1 min</span>
            </span>
            <!-- Categories -->
            
              <!-- <span class="text-gray-400">·</span> -->
              <span class="flex flex-row items-center gap-1 group hover:underline">
                <iconify-icon class="transition-all group-hover:scale-125 mr-0"
                  width="18"
                  icon="mingcute:classify-2-fill">
                </iconify-icon>
                <a class="article-category-link" href="/categories/%E7%A0%94%E7%A9%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B/">研究-大模型</a>
              </span>
            
          </div>
        </section>
      </div>
      <!-- tags -->
      <div>
        
<div class="flex flex-wrap gap-1">
  
    
      <a href="/tags/research/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        research
      </a>
    
      <a href="/tags/llm/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        llm
      </a>
    
  
</div>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto dark:prose-invert">
    <h1 id="Calculation-Logits"><a href="#Calculation-Logits" class="headerlink" title="Calculation-Logits"></a>Calculation-Logits</h1><h2 id="ACL2024-Long-Paper-Shifting-Attention-to-Relevance-Towards-the-Predictive-Uncertainty-Quantification-of-Free-Form-Large-Language-Models"><a href="#ACL2024-Long-Paper-Shifting-Attention-to-Relevance-Towards-the-Predictive-Uncertainty-Quantification-of-Free-Form-Large-Language-Models" class="headerlink" title="[ACL2024 Long Paper] Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models"></a>[ACL2024 Long Paper] Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models</h2><p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20241110154603.png" alt="token 加权熵"></p>
<p><strong>Motivation</strong></p>
<p>与 prompt 相关的 token 在不确定性估计中是否比不相关的 token 更为关键?</p>
<p>论文以 <code>Predictive Entropy</code> 为 Baseline, 通过引入 <code>Relevance</code> 和 <code>Uncertainty Proportion</code> 来回答上面的问题. 前者用来衡量 token 或句子语义相关度, 越高代表语义越重要; 后者用来衡量 token 或句子的不确定性, 越高代表承载的不确定性价值越高</p>
<p>Predictive Entropy 的计算公式为</p>
<script type="math/tex; mode=display">
PE(\boldsymbol{s}, \boldsymbol{x}) = -\log p(\boldsymbol{s}|\boldsymbol{x}) = -\sum_{i}\log p(z_i|\boldsymbol{s}_{<i}, \boldsymbol{x})</script><p>在 token 级别, 有</p>
<script type="math/tex; mode=display">
\begin{aligned}
R_T(z_i, \boldsymbol{s}, \boldsymbol{x}) &= 1-|g(\boldsymbol{x}\cup\boldsymbol{s}, \boldsymbol{x}\cup\boldsymbol{s}\backslash \{z_i \})|\\

UP_T(z_i, \boldsymbol{s}, \boldsymbol{x}) &= \frac{-\log p(z_i|\boldsymbol{s}_{<i}, \boldsymbol{x})}{PE(\boldsymbol{s}, \boldsymbol{x})}
\end{aligned}</script><blockquote>
<p>公式里的 “\“ 代表集合的差集</p>
</blockquote>
<p>这里 $\mathbf{x}$ 代表输入的 prompt, $z_i$ 代表第 $i$ 个 token, $\boldsymbol{s}$ 代表句子, $\mathbf{x}$ 代表输入, $g(\cdot, \cdot)$ 计算两个句子的相似度, 具体而言论文使用 Cross-Encoder-RoBERTa-large 方法</p>
<p>在 sentence 级别, 有</p>
<script type="math/tex; mode=display">
\begin{aligned}
R_S(\boldsymbol{s}, S, \boldsymbol{x}) &= \sum_{j=1, j\neq i} g(\boldsymbol{s}_i, \boldsymbol{j})p(\boldsymbol{s}_j|\boldsymbol{x})\\

UP_S(\boldsymbol{s}, S, \boldsymbol{x}) &= \frac{PE(\boldsymbol{s}_i, \boldsymbol{x})}{\sum_k PE(\boldsymbol{s}_k, \boldsymbol{x})}

\end{aligned}</script><p>这里 $S$ 代表包括所有句子的集合, $\boldsymbol{s}$ 代表句子, $p(\boldsymbol{s}_j|\boldsymbol{x})$ 代表句子 $\boldsymbol{s}_j$ 的概率, $1\leq k\leq K$, $1\leq i,j\leq K$</p>
<blockquote>
<p>直观上看, token-level 的 Relevance 反映了 token $z_i$ 在句子 $\boldsymbol{s}$ 中的语义重要性, 计算了去除该 token 前后句子语义的变化幅度, 再用 1 减去上值; Uncertainty Proportion 反映了 token $z_i$ 承载的不确定性价值, 计算了 token $z_i$ 的朴素熵占整个句子不确定性的比例. sentence-level 同理</p>
</blockquote>
<p>依次计算每个 token 或句子的 Relevance 和 Uncertainty Proportion, 得到如下结果: 仅有少量 token 具有较大的 Relevance 和 Uncertainty Proportion; Relevance 越小, Uncertainty Proportion 越大, 说明承载语义信息少的 token 承载的不确定性价值更高 (某种二次证明, 符合直觉)</p>
<p>Conclusion: 与 prompt 相关的 token 在不确定性估计中比不相关的 token 更为关键, 应当给予更高的权值</p>
<p><strong>Method</strong></p>
<p><strong>token-level</strong></p>
<p>Shifting Attention to Relevance (SAR). 定义句子 $\boldsymbol{s}<em>j = {z_1, z_2, \dots, z</em>{N_j}  }$, $N_j$ 表示句子 $\boldsymbol{s}_j$ 的长度. 基于 Motivation 部分的公式, 将句子内部不同 token 的 Relevance 进行归一化. 包含 $N_j$ 个 token 的句子 $\boldsymbol{s}_j$ 的 Relevance 为</p>
<script type="math/tex; mode=display">
\tilde{R}_T(z_i, \boldsymbol{s}_j, \boldsymbol{x}) = \frac{R_T(z_i, \boldsymbol{s}_j, \boldsymbol{x})}{\sum_{n}^{N_j} R_T(z_n, \boldsymbol{s}_j, \boldsymbol{x})}</script><p>然后向语义重要的 token 增加权值, 修改句子的 Predictive Entropy 计算公式为</p>
<script type="math/tex; mode=display">
E_T(z_i, \boldsymbol{s}_j, \boldsymbol{x}) = -\log p(z_i|\boldsymbol{s}_{<i}, \boldsymbol{x})\tilde{R}_T(z_i, \boldsymbol{s}_j, \boldsymbol{x})</script><p>于是得到整个段落进行 token 语义偏移后的熵为</p>
<script type="math/tex; mode=display">
\mathrm{TOKEN}\mathit{SAR}(\boldsymbol{s}_j, \boldsymbol{x}) = \sum_{i}^{N_j} E_T(z_i, \boldsymbol{s}_j, \boldsymbol{x})</script><p><strong>sentence-level</strong></p>
<p>语义上一致的句子比其他句子更具说服力, 所以可以通过提高语义一致的句子的生成概率来减少句子的不确定性</p>
<script type="math/tex; mode=display">
\begin{aligned}
E_S(\boldsymbol{s}_j, S, \boldsymbol{x}) &= -\log \left( p(\boldsymbol{s}_j|\boldsymbol{x}) + \frac{1}{t}R_S(\boldsymbol{s}_j, S, \boldsymbol{x})\right)\\

&= -\log \left(p(\boldsymbol{s}_j|\boldsymbol{x}) + \underbrace{\frac{\sum_{k\neq j}g(\boldsymbol{s}_j, \boldsymbol{s}_k)p(\boldsymbol{s}_k|\boldsymbol{x})}{t}}_{\text{sentence relevance}}\right)

\end{aligned}</script><p>这里 $p(\boldsymbol{s}_j|\boldsymbol{x}) = \prod_i p(z_i|\boldsymbol{&lt;i}, \boldsymbol{x}$ 是句子 $\boldsymbol{s}_j$ 的生成概率, $t$ 控制偏移幅度的温度系数 (代码里默认是 0.001). 于是得到整个段落进行 sentence 语义偏移后的熵为 ($K$ 为段落中的句子数)</p>
<blockquote>
<p>sentence-level 进行偏移的方法比较粗糙. 计算单个句子的熵时, 作者在句子出现的概率上加了一个有关于 Relevance 的项, 通过这一项的温度系数来控制偏移幅度<br>为什么这个温度系数可以控制偏移幅度呢? 这里公式是熵的形式, 真数 (也就是对数函数的自变量处) 进行线性增长, 线性增长的系数控制函数的增长速度. 在该语境下, (对单一句子而言的) 语义一致的句子的 Relevance 本身更大, 从而乘系数后得到的增幅更大, 进而与语义不一致的句子的熵的差值更大 (感觉说的有点糊涂了, 下面的例子更直接吧)<br>类比, 2*10=20 和 4*10=40, 虽然乘数相同, 但是结果差距变得更大了, 从而增加了真数部分的差值, 进而提高了语义一致句子的熵与语义不一致句子的熵的差值</p>
</blockquote>
<script type="math/tex; mode=display">
\mathrm{SENT}\mathit{SAR}(S, \boldsymbol{x}) = \frac{1}{K}\sum_k E_S(\boldsymbol{s}_k, S, \boldsymbol{x})</script><blockquote>
<p>值得注意的是, sentence-level 计算单一句子的熵形式与语义熵相似, 本文作者提出语义熵 (ICLR2023) 在平均超过 20 个 token 的长句中表现不理想, 而本方法会更好. orz, 平均超过 20 个token, 这个长度有点长了?</p>
<p>语义熵的计算公式如下</p>
<script type="math/tex; mode=display">
\begin{aligned}
SE(x) &= -\sum_c p(c|x)\log p(c|x)\\
&= -\sum_c \left(\left(\sum_{\mathbf{s}\in c} p(\mathbf{s}|x)\right) \log\left[\sum_{\mathbf{s}\in c} p(\mathbf{s}|x)\right]   \right)
\end{aligned}</script><p>主要区别在于, 负对数概率乘的权值不同, 语义熵是直接乘以句子的生成概率, 而本文是乘以句子的 Relevance</p>
</blockquote>
<p><strong>Overall Measurement</strong></p>
<p>token-level 和 sentence-level 的两种偏移处理方法各有不同, 但他们是正交的, 可以自然地结合起来. 为此, 只需将 sentence-level 方法的每个句子的生成概率替换成 token-level 方法的偏移概率即可. 也即</p>
<script type="math/tex; mode=display">
\begin{aligned}
p'(\boldsymbol{s}_i|\boldsymbol{x}) &= e^{-\mathrm{TOKEN}\mathit{SAR}(\boldsymbol{s}_i, \boldsymbol{x})}\\
p'(\boldsymbol{s}_j|\boldsymbol{x}) &= e^{-\mathrm{TOKEN}\mathit{SAR}(\boldsymbol{s}_j, \boldsymbol{x})}\\
E_{T,S}(\boldsymbol{s}_j, S, \boldsymbol{x}) &= -\log (p'(\boldsymbol{s}_i|\boldsymbol{x}) + \frac{\sum_{k\neq j}g(\boldsymbol{s}_j, \boldsymbol{k})p'(\boldsymbol{s}_j|\boldsymbol{x})}{t}
\end{aligned}</script><p>因此, $K$ 个句子的 token- and sentence-level shifted predictive entropy 为 </p>
<script type="math/tex; mode=display">
\mathit{SAR} = \frac{1}{K}\sum_k E_{T, S}(\boldsymbol{s}_k, S, \boldsymbol{x})</script><blockquote>
<p>附录关于温度系数 $t$ 的消融实验表明, 使用 $t=1\times10^{-3}$ 和 $t=1\times10^0$ 区别不大. 感觉这个 sentence-level 的偏移方法还是粗糙了一些, 可以优化.</p>
</blockquote>
<h2 id="ICLR2025-Workshop-How-Many-Opinions-Does-Your-LLM-Have-Improving-Uncertainty-Estimation-in-NLG"><a href="#ICLR2025-Workshop-How-Many-Opinions-Does-Your-LLM-Have-Improving-Uncertainty-Estimation-in-NLG" class="headerlink" title="[ICLR2025 Workshop] How Many Opinions Does Your LLM Have? Improving Uncertainty Estimation in NLG"></a>[ICLR2025 Workshop] How Many Opinions Does Your LLM Have? Improving Uncertainty Estimation in NLG</h2><p>语义熵使用多项式采样, 多项式采样会倾向于生成多个序列的相似样本, 这在计算上是低效的, 且一些语义类可能被忽略. 基于此, 论文提出 SDLG (Semantic Diverse Language Generation), 搜索那些具有高概率且具有高语义多样性的输出序列. 论文代码未开源</p>
<p>具体而言, 文章提出计算 Attribution score, Substitution score, Importance score. Attribution score 根据推理的梯度, 确定哪个输出序列中的 token 应该被更改; 该分数表示当 token $y_i$ 被更改时, 对语义的影响越大. Substitution score 表示用词表的哪个 token 替换更好; 该分数越高表示 Embedding 的变化与语义变化更一致. Importance score 表示在给定上下文情况下, 用替代 token 替换当前 token 的可能性; 该分数越高表示替换当前 token 时的可能性越大/重要性越高. </p>
<p>计算完所有替代 token $v_i$ 的三个分数后, 根据分数对潜在替代项排序, 然后用排名最高的替代 token 替换既有生成 token, 然后按原有解码方式 (一般式多项式采样) 继续生成过程.</p>
<h1 id="Calculation-Internal-State"><a href="#Calculation-Internal-State" class="headerlink" title="Calculation-Internal State"></a>Calculation-Internal State</h1><h2 id="ICML2024-Poster-Characterizing-Truthfulness-in-Large-Language-Model-Generations-with-Local-Intrinsic-Dimension"><a href="#ICML2024-Poster-Characterizing-Truthfulness-in-Large-Language-Model-Generations-with-Local-Intrinsic-Dimension" class="headerlink" title="[ICML2024 Poster] Characterizing Truthfulness in Large Language Model Generations  with Local Intrinsic Dimension"></a>[ICML2024 Poster] Characterizing Truthfulness in Large Language Model Generations  with Local Intrinsic Dimension</h2><p>过去一些文章研究了神经网络的内在维度 Intrinsic Dimension, 本篇文章基于过去的一些工作在 LLM 的 Local Intrinsic Dimension (LID) 上进一步研究</p>
<p><strong>Problem Setup</strong></p>
<p>考虑 Casual LLM $M$, 含有 $N$ 个 token 的输入序列 $X = [x<em>1, x_2, \dots, x_N]$, 含有 $O$ 个 token 的输出序列 $M(X) = [x</em>{N+1}, x<em>{N+2}, \dots, x</em>{N+O}]$. 这里输出 token $x<em>{N+i}, i\in[1, \dots, O]$ 等价为在前缀 $x_1, x_2, \dots, x_N, x</em>{N+1}, \dots, x_{N+i-1}$ 下从模型词表 $\mathcal{V}$ 中采样的结果</p>
<script type="math/tex; mode=display">
\mathbf{X} = (M_L \circ M_{L-1} \circ \cdots \circ M_0)(x_1, dots, x_{N+i-1})\\

p(x_{N+i}|[x_1, \dots, x_{N+i-1}]) = \mathrm{softmax}(W\mathbf{X}_{Li}+b)</script><p>这里 $M<em>j, j\in [1, \dots, L]$ 是 LLM 的第 $j$ 层, $M_0$ 是 Embedding 层, $W$ 和 $b$ 是输出层的权重和偏置, $\mathbf{X}</em>{Li}$ 是第 $j$ 层的输出, 后续用 $\mathbf{X}<em>{ji}$  表示第 $j$ 层 Embedding 输出的第 $i$ 个 token $x</em>{N+i}$ (是一个 $D$ 维向量), $p(x<em>{N+i}|\cdot)$ 表示单一 token $x</em>{N+i}$ 的分布, 同理 $p(M(X))$ 表示 $M(X)$ 的分布.</p>
<p>考虑有 $n$ 个样本的数据集 $\mathcal{D} = \left{X^1, \dots, X^n\right}$, 我们想得到一一对应的数据集可信度集合 $\left{M(X^1), \dots, M(X^n)\right}$. 这里使用 $\left{\hat{Y}^1, \dots, \hat{Y}^n \right}$ 表示真实值, 使用 $s\left(M(X^i), \hat{Y}^i \in {0, 1} \right)$ 作为输出是否可信的指示函数.</p>
<p><strong>以往工作大多从预测分布 $p(M(X)|\cdot)$ 中获取特征, 本文从中间表示 $\mathbf{X}_{ji}$ 的 LID 中进行探索</strong></p>
<p><strong>MLE Estimator for LID</strong></p>
<blockquote>
<p>酸爽的前置知识学习环节</p>
</blockquote>
<p>MLE estimator for LIDs 是2004年的工作, 用于估计单个 point(点) 的 local instrisic dimension (LID, 局部内在维度), 这与其他 global 估计的方法不同</p>
<p>以下的讨论中, 我们选定模型中间某一层的最后一个 token, 从而在讨论中忽略 layer 和 token 的下标</p>
<p>对于数据集 $\mathcal{D}$ 中的一个数据点 $\mathbf{X}^i$, MLE 估计器将泊松过程拟合到其近邻点的数量上. 形式上, 考虑数据点 $\mathbf{X}^i$ 的 $T$ 个最近邻点 ${\mathbf{X}^{i1}, \dots, \mathbf{X}^{iT}}$; 考虑 $\mathbf{X}^i$ 为中心, 半径为 $R$ 的一个球 $S_{\mathbf{X}^i}(R)$; 在半径 $0&lt;t&lt;R$ 的球内近邻点数量可以表示为以下二项过程</p>
<script type="math/tex; mode=display">
N(t, \mathbf{X}^i) = \sum_{k=1}^T\mathbb{I}\{\mathbf{X}^{ik}\in S_{\mathbf{X}^i}(t)  \}</script><p>上述过程可以用速率为 $\lambda(t)$ 的泊松过程来近似, 假设密度 $f$ 在点 $\mathbf{X}^i$ 附近近似恒定, 并且体积 $V$ 与 $t^m$ 成比例扩大 $V = V_mt^m$, 这里 $m$ 是内在维度的数量. 于是有</p>
<script type="math/tex; mode=display">
\lambda(t) = f\frac{\mathrm{d}V}{\mathrm{d}t} = fV_mt^{m-1}</script><blockquote>
<p>泊松过程涉及到随机过程这门课, 放弃之</p>
</blockquote>
<p>所以泊松过程的对数似然可以写成内在维度数 $m$ 和 $\theta = \log f$ 的函数</p>
<script type="math/tex; mode=display">
L(m, \theta) = \int_0^R\log \lambda(t)\mathrm{d}(N_{\lambda}(t)) - \int_0^R\lambda(t)\mathrm{d}t</script><p>最大化上述对数似然函数, 得到计算内在维度 $m$ 的公式</p>
<script type="math/tex; mode=display">
m(R, \mathbf{X}^i) = \left(\frac{1}{N(R, \mathbf{X}^i)} \sum_{j=1}^{N(R, \mathbf{X}^i)} \log\frac{R}{Q_j} \right)^{-1}</script><p>这里 $Q_j, j=1, \dots, T$ 是中心点 $\mathbf{X}^i$ 与第 $j$ 个近邻点 $\mathbf{X}^{ij}$ 之间的距离. 上式可以进一步简化为</p>
<script type="math/tex; mode=display">
m(\mathbf{X}^i) = \left(\frac{1}{T-1} \sum_{j=1}^{T-1} \log\frac{Q_T}{Q_j} \right)^{-1}</script><blockquote>
<p>并未手动推导上述公式, </p>
</blockquote>
<p><strong>Layer Selection and Distance-aware MLE</strong></p>
<p>在上一节, 讨论了如何计算 $\mathbf{X}^i$ 的 LIDs. 在大模型中计算的话, 存在两个问题</p>
<ol>
<li>每一层的表示维度为 $D$, 所以很难说用哪个是最优的</li>
<li>MLE 假定密度函数 $f$ 是恒定的, 这在 Causal LLMs 不成立</li>
</ol>
<p>解决方法有两种</p>
<p><strong>Layer Selection</strong></p>
<p>LLMs 在每层为每个 token 生成一个 $D$ 维的表示. 这里选择 $\mathbf{X}^i$ 的最后一个 token 即 $\mathbf{X}^i_{-1}$ 作为这一层的表示</p>
<p>但是实验证明, 最后一层 layer 的表示可能不是最具信息量的特征 (LIDs 预测真实性的性能与测试集上 LIDs 绝对值的总和有很好的相关性, 但在某些层上有所偏移). 这里建议选择 $l$ 层</p>
<script type="math/tex; mode=display">
l = \argmax_l\sum_{i=1}^n m\left(\mathbf{X}_{l\{-1\}}^i \right) + 1</script><p><strong>Distance-aware MLE</strong></p>
<p>为了缓解泊松过程做最大似然估计时的密度不均匀问题, 可以调整泊松过程的速率 $\lambda(t)$. 既有研究指出, 可以用 $\hat{\lambda}(t) = fV_mmt^{m-1} + t^mV_m\delta(t)$ 替代 $\lambda(t) = fV_mmt^{m-1}$, 这里 $\delta(R)$ 是受 $\mathbf{X}^i$ 几何性质限制的修正函数, 其具体形式参加原论文 (雾)</p>
<p>有了新的速率, 可以重新进行最大似然估计, 对结果进行泰勒展开, 再使用多项式回归计算修正, 然后然后</p>
<blockquote>
<p>卒. 评价是不是我现在能快速看懂的论文</p>
</blockquote>
<h1 id="Blackbox"><a href="#Blackbox" class="headerlink" title="Blackbox"></a>Blackbox</h1><h2 id="ICML2024-Oral-Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling"><a href="#ICML2024-Oral-Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling" class="headerlink" title="[ICML2024 Oral] Decomposing Uncertainty for Large Language Models  through Input Clarification Ensembling"></a>[ICML2024 Oral] Decomposing Uncertainty for Large Language Models  through Input Clarification Ensembling</h2><blockquote>
<p>一句话概括: 本文聚焦于偶然性不确定性, 提出了一种和 Deep Emsemble 对称的不确定性度量方式, 通过对输入进行”澄清”, 从而减少输入的偶然不确定性. Whitebox 和 Blackbox 均可以使用</p>
</blockquote>
<p>Uncertainty Decomposition 是将预测模型的不确定性分解为偶然不确定性(数据不确定性)和认知不确定性(模型不确定性), 前者是数据生成汇总固有的随机性引起的, 后者是模型训练数据缺失引起的. 既有的 UD 研究主要研究偶然不确定性</p>
<p>$\boldsymbol{X}$ 和 $\boldsymbol{Y}$ 代表任务的输出输出, $\theta$ 代表 LLM 的参数; $p(\boldsymbol{Y}|\boldsymbol{X})$ 和 $q(\boldsymbol{Y}|\boldsymbol{X}, \boldsymbol{\theta}$ 分别代表真实分布和模型分布. 总不确定性 total uncertainty 是预测分布的熵 $\mathcal{U} = \mathcal{H}(q(\boldsymbol{Y}|\boldsymbol{X; \boldsymbol{\theta}}))$, 由偶然不确定性 aleatoric uncertainty $\mathcal{U}_D$ 和 认知不确定性 epistemic uncertainty $\mathcal{U}_E$ 组成</p>
<p><strong>贝叶斯神经网络 Bayesian Neural Network(2015) 和深度集成 Deep Ensembles(2017)</strong></p>
<p>贝叶斯神经网络 (BNN) 对神经网络的参数分布进行建模, 给定训练数据, 可以通过变分推断近似后验分布. 这种方法计算成本很高; </p>
<p>深度集成有更好的扩展性, 维护 K 个模型, 每个模型的参数为 $\boldsymbol{\theta}^{(k)}$, 最小化每个模型训练的损失函数, 相当于完成优化问题 $\min \mathrm{KL}(p(\boldsymbol{Y}|\boldsymbol{X})|q(\boldsymbol{Y}|\boldsymbol{X}, \boldsymbol{\theta})).$, 不同模型的初始值不同, 优化后的参数也不同, 所有模型的参数可看成采样结果, 根据采样结果估计参数的分布 $p(\boldsymbol{\theta}|\mathcal{D})$, 这里 $\mathcal{D}$ 是训练数据集. 所以贝叶斯网络的集成分布可以表示为 $q(\boldsymbol{Y}|\boldsymbol{X}) = \mathbb{E}_{q(\boldsymbol{\theta}|\mathcal{D})} [q(\boldsymbol{Y}|\boldsymbol{X}, \boldsymbol{\theta})].$, 所以不确定量可以表示为</p>
<script type="math/tex; mode=display">
\mathcal{H}(q(\boldsymbol{Y}|\boldsymbol{X}) = \underbrace{\mathcal{I}(\boldsymbol{Y}; \boldsymbol{\theta}|\boldsymbol{X})}_{\textcircled1} + \underbrace{\mathbb{E}_{q(\boldsymbol{\theta}|\mathcal{D})} [\mathcal{H}(q(\boldsymbol{Y}|\boldsymbol{X}, \boldsymbol{\theta}))]}_{\textcircled2}</script><p>这里 $\mathcal{L}$ 是在 $q$ 分布下的互信息. 第一项用于衡量不同模型见的分歧, 第二项用来衡量单独模型的平均不确定性, 两项可以近似于认知不确定性和偶然不确定性</p>
<p>上述两种方法常用来衡量模型的不确定性, 但是对 LLMs 是否依然适合呢? 作者做了实验, 发现深度集成并没有显著减小认知不确定性, 所以这两种方法失效了. 进一步的, 作者提出了一种对称于 BNN 方法的替代框架</p>
<p><strong>Input Clarification Ensembling</strong></p>
<p>既然集成模型以减小认知不确定性比较困难, 那能修改输入来减小认知不确定性吗?</p>
<blockquote>
<p>Excuse? 这样减小的不是偶然不确定性吗</p>
</blockquote>
<ol>
<li>Input Clarification: 给定输入 $\boldsymbol{X}$, 生成一系列 “澄清”(clarification) $\boldsymbol{C}^{(k)}$, 这些澄清用于减小输入带来的认知不确定性. 加入澄清后的输入表示为 $\boldsymbol{X}\oplus \boldsymbol{C}^k$, 这里 $\oplus$ 代表 concat 操作. 需要说明的是, ${\boldsymbol{C}^{(k)}}$ 是一个集合</li>
<li>Ensemble: 输入澄清的分布表示为 $q(\boldsymbol{C}|\boldsymbol{X})$, 模型输出变化为 $q(\boldsymbol{Y}|\boldsymbol{X}) = \mathbb{E}_{q(\boldsymbol{C}|\boldsymbol{X})}[q(\boldsymbol{Y}|\boldsymbol{X}\oplus\boldsymbol{C}, \boldsymbol{\theta})]$<script type="math/tex; mode=display">\mathcal{H}(q(\boldsymbol{Y}|\boldsymbol{X})) = \underbrace{\mathcal{I}(\boldsymbol{Y}; \boldsymbol{C}|\boldsymbol{X})}_{\textcircled1'} + \underbrace{\mathbb{E}_{q(\boldsymbol{\boldsymbol{C}|\boldsymbol{X}})}\mathcal{H}(q(\boldsymbol{Y}|\boldsymbol{X}\oplus \boldsymbol{C}))}_{\textcircled2'}</script>上面 $\textcircled1’$ 代表模型输出分布与澄清间的互信息, 可以近似输入模糊性引起的偶然不确定性; $\textcircled2’$ 代表给定不同澄清时的模型输出分布的平均熵, 表示给定澄清后的不确定性, 也即认知不确定性 (这里假设输入模糊性是认知不确定性的唯一来源)</li>
</ol>
<p>值得注意的是, 以上公式与 Deep Ensemble 的公式具有对称性, 二者分别关注偶然不确定性和认知不确定性, 这分别对应各自公式的第二项</p>
<p><strong>Input Clarification</strong></p>
<p>本文关注由指令模糊性和问题模糊性引起的不确定性, 提出了一个框架</p>
<blockquote>
<p>作者表明, 该框架可以很容易地推广到其他输入组件, 如 Instructions, In-Context Examples, Questions. 与这里的整体输入是类似的. (究竟如何, 谁知道呢)</p>
</blockquote>
<p>工作的关键在于 clarification 的选择, 文中用一个 clarification LLM 来生成, 所以 $q(\boldsymbol{C}|\boldsymbol{X})$ 其实是 clarification LLM 的输出分布</p>
<blockquote>
<p>实际上做的工作类似于, 用户输入指令后, 另一个 LLM 判断下用户的指令是否清晰, 不清晰则生成澄清指令, 从而减小模型的认知不确定性; 或者像作者在 improving performance 里说的, 让用户重新输入</p>
</blockquote>
<h2 id="TMLR202405-Generating-with-Confidence-Uncertainty-Quantification-for-Black-box-Large-Language-Models"><a href="#TMLR202405-Generating-with-Confidence-Uncertainty-Quantification-for-Black-box-Large-Language-Models" class="headerlink" title="[TMLR202405] Generating with Confidence: Uncertainty Quantification for  Black-box Large Language Models"></a>[TMLR202405] Generating with Confidence: Uncertainty Quantification for  Black-box Large Language Models</h2><p>这篇文章指出, 既有研究忽略了不确定性和置信度间的区别, 作者认为置信度是衡量给定响应正确性的更可靠指标</p>
<ul>
<li>置信度: 对特定预测/生成的置信度</li>
<li>不确定性: 对固定输入的潜在预测的 “分散程度”</li>
</ul>
<blockquote>
<p>举个栗子, 对于正态分布 $P(Y|x) = \mathcal{N}(\mu, \sigma^2)$, 方差 $\sigma^2$ 可以看成不确定的度量, 对于特定预测 $Y=y$, $-\frac{|y-\mu|}{\sigma}$ 可以看成 confidence 的度量 (或称置信度). 最常用生成序列的联合概率来表示生成的置信度, 表示如下</p>
<p>$C(x, \boldsymbol{s}) = \hat{p}(\boldsymbol{s}|x) = \prod<em>i \hat{p}(s_i|s</em>{&lt;i}, x)$</p>
<p>Existing literature sometimes uses uncertainty estimate U (x) to predict the correctness of a particular response s (Kuhn et al., 2023; Malinin &amp; Gales, 2021), ignoring the distinction between uncertainty and confidence. Section 5 shows that this is problematic, and confidence is a more reliable indicator of the correctness of a given response.</p>
</blockquote>
<p>论文将模型采样多条答案作为前提, 作者将当前衡量答案相似度的方法分为两类</p>
<ol>
<li>Jaccard Similarity. 具体计算方法是, 两个集合交集的元素数量除以并集的元素数量; 句子/文档视为单词的集合; 这种方法计算效率较高, 但会忽略词序, 无法捕捉否定词的关键信息<br>$a<em>{Jaccard}(\boldsymbol{s}</em>{j<em>1}, \boldsymbol{s}</em>{j<em>2}) = |\boldsymbol{s}</em>{j<em>1}\cap \boldsymbol{s}</em>{j<em>2}|/|\boldsymbol{s}</em>{j<em>1}\cap \boldsymbol{s}</em>{j_2}|\in[0, 1]$</li>
<li>Natural Language Inference(NLI). 现有的工作大多基于语义熵的 DeBERTa-large 模型来做, NLI 模型预测出 entailment(蕴含), neutral(中立), contradiction(矛盾) 三种答案间的关系</li>
</ol>
<p>以以上两种相似度度量方法为基础, 作者进一步提出置信度的计算方法, 这里第二种方法也就是作者提出的方法</p>
<ol>
<li>Number of Semantic Sets. 也就是语义熵使用的方法, 计算方法表示为 $U_{\mathtt{NumSet}}$</li>
<li><p>Sum of Eigenvalues of the Graph Laplacian. 实际上, 类别的区分不一定是非黑即白的, 而且 NLI 模型判断的方法不一定有语义等价上的传递性, 我们需要一种更细致且”连续”的方法.</p>
<p>只知道 Similarity 但不知道 Embedding, 所以可以选择谱聚类 (spectual clustering) 的方法. 将每个答案视作一个节点, 定义加权邻接矩阵 $W = (w<em>{j_1, j_2})</em>{j<em>1, j_2}=1, \dots, m$, 这里 $w</em>{j<em>1, j_2} = (a</em>{j<em>1, j_2}+a</em>{j_2, j_1})/2$, 对称归一化图拉普拉斯算子为</p>
<script type="math/tex; mode=display">
L := I-D^{-1/2}WD^{-1/2}\\
D_{j_1, j_2} = \begin{cases}
\sum_{j'\in[m]}w_{j_1, j'} & (j_1=j_2)\\
0 & (j_1\neq j_2)
\end{cases}</script><p>语义熵里的与依据类方法是离散的(非黑即白), 其连续化的版本可以用 $L$ 的特征值 $\lambda_1&lt;\cdots&lt;\lambda_m$ 表示</p>
<script type="math/tex; mode=display">
U_{\mathtt{EigV}} = \sum_{k=1}^{m}\max(0, 1-\lambda_k)</script><blockquote>
<p>证明略</p>
</blockquote>
</li>
</ol>
<h1 id="Self-Correct"><a href="#Self-Correct" class="headerlink" title="Self Correct"></a>Self Correct</h1><h2 id="ICLR2024-poster-Large-Language-Models-Cannot-Self-Correct-Reasoning-Yet"><a href="#ICLR2024-poster-Large-Language-Models-Cannot-Self-Correct-Reasoning-Yet" class="headerlink" title="[ICLR2024 poster] Large Language Models Cannot Self-Correct Reasoning Yet"></a>[ICLR2024 poster] Large Language Models Cannot Self-Correct Reasoning Yet</h2><p>intrinsic self-correction: 不依靠外部反馈, 模型仅依靠其固有能力纠正初始回答</p>
<p>作者提出, 既有 intrinsic self-correction 方法有效果得益于</p>
<ol>
<li>使用真实标签作为监督信号. Self Correction 的方法使用真实标签作为监督信号, 当模型回答正确时停止修正</li>
<li>和 self-consistency 方法错误地比较. 将模型答案喂给模型自身, 这与模型生成答案的 “自一致性” 有关, 而不是 self correction 的关系</li>
<li>未选择最优 prompt 设计. 没有要求初始回答的 prompt 设计有缺陷, 从而导致 self correction 效果提升</li>
</ol>
<p>文章认为, 模型在没有外部反馈时, 不能做到 self correction. 附录给的全是例子</p>
<h2 id="EMNLP2024-Findings-Think-Twice-Before-Trusting-Self-Detection-for-Large-Language-Models-through-Comprehensive-Answer-Reflection"><a href="#EMNLP2024-Findings-Think-Twice-Before-Trusting-Self-Detection-for-Large-Language-Models-through-Comprehensive-Answer-Reflection" class="headerlink" title="[EMNLP2024 Findings] Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection"></a>[EMNLP2024 Findings] Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection</h2><p>Self Evaluation 的范式是将生成的多个答案重新输出模型让模型判断可信度, 是一种类似于 Model Ensemble 的方法. 本文在将模型生成的 Answer 重新输入模型进行判断前, 为每个答案生成 justification, 类似进行了 Joint Confidence Calibration, 再输出 GPT. 这种方法作者自称 Comprehensive Answer Reflection. 论文没有给代码</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20241127154215.png" alt="Think Twice Before Trusting"></p>
<p>作者还给了一个 Self Detection 和他们方法范式的对比</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20241127154801.png" alt="Self Detection 与 Comprehensive Answer Evaluation Paradigm 的对比"></p>
<h2 id="EMNLP2023-Main-Large-Language-Models-Can-Self-Improve"><a href="#EMNLP2023-Main-Large-Language-Models-Can-Self-Improve" class="headerlink" title="[EMNLP2023 Main] Large Language Models Can Self-Improve"></a>[EMNLP2023 Main] Large Language Models Can Self-Improve</h2><p>Few-Shot 设置,  CoT + Self Consistency 从而得到无标签数据集的标签, 用这个这些数据 finetuning 模型, 可以提高模型性能. 代码未开源</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20241127160116.png" alt="20241127160116"></p>

  </article>

  <!-- prev and next -->
  <div class="flex justify-between mt-4 pt-4
    border-t border-[var(--c-sep)] text-sm
    gap-2 text-[var(--c-50)]
  ">
    <div>
      
        <a href="/2024/12/04/tools/git-commands/"
          class="
            transition-all
            flex justify-center
            hover:-translate-x-1
            hover:text-[var(--c-80)]
          ">
          <iconify-icon width="20" icon="mingcute:left-fill" data-inline="false">
          </iconify-icon>
          git 命令速查
        </a>
      
    </div>
    <div>
      
        <a href="/2024/11/07/llm/InternLM-Practice/InternLM-Practice-Note/"
          class="
            flex 
            justify-center
            hover:translate-x-1 
            transition-transform
            hover:text-[var(--c-100)]
          "
        >
          书生大模型训练营笔记
          <iconify-icon width="20" icon="mingcute:right-fill" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>

  <!-- comment -->
  <div class="article-comments mt-12">
    
  <script src="https://giscus.app/client.js"
  data-repo="421zuoduan/blog-giscus-discussion"
  data-repo-id="R_kgDONKEKag"
  data-category="Announcements"
  data-category-id="DIC_kwDONKEKas4Cj9R8"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="1"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="zh-CN"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>
<script>
  window.onload = function () {
    console.log("giscus loaded");
    const isDark = document.documentElement.classList.contains("dark");
    const giscusFrame = document.querySelector("iframe.giscus-frame");
    giscusFrame.contentWindow.postMessage(
      {
        giscus: {
          setConfig: {
            theme: isDark ? "dark" : "light",
          },
        },
      },
      "https://giscus.app"
    );
  };
</script>


  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        var title = this.title;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        if (title) $(this).after('<span class="fancybox-title">' + title + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->

  </main>
  <footer class="flex flex-col mt-18 mb-12 items-center
  text-[var(--c-50)] text-sm">
  <div class="flex flex-row items-center my-12">
    
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="Github" target="_blank" rel="noopener" href="https://www.github.com/421zuoduan">
            <iconify-icon width="28" icon="mingcute:github-fill"></iconify-icon>
        </a>
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="ZhiHu" target="_blank" rel="noopener" href="https://www.zhihu.com/people/ren-jian-lan-xue">
            <iconify-icon width="28" icon="ri:zhihu-line"></iconify-icon>
        </a>
    

  </div>
  <!-- busuanzi -->
  <div class="mb-6">
    
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex flex-col items-center mb-2">
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="ic:round-person" width="18"></iconify-icon>
    <span class="mr-1">访客 Visitors: </span>
    <span id="busuanzi_value_site_uv"></span>
  </div>
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="carbon:view-filled" width="18"></iconify-icon>
    <span class="mx-1">浏览量 Page Views:</span>
    <span id="busuanzi_value_site_pv"></span>
  </div>
</div>
<!-- End Busuanzi Analytics -->


  </div>
  <!-- copyright -->
  <div class="flex flex-row items-center gap-2">
    <a class="hover:underline"
      target="_blank"
      href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
    >
      CC BY-NC-SA 4.0
    </a>
    <span>© 2022-2024</span>
    <a class="hover:underline"
    href="https://github.com/chen-yingfa" 
    target="_blank" 
    rel="noopener noreferrer">陈英发</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-1">
    <span>Powered by</span>
    <a class="hover:underline" 
    href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/chen-yingfa/hexo-theme-fengye" 
    class="hover:underline"
    target="_blank"
      rel="noopener noreferrer"
      >
      枫叶 Fengye
    </a>
  </div>

</footer>

  <div class="
    back-to-top
    fixed right-6
    z-1024
    -bottom-20
    rounded-lg
    font-bold
    py-1 px-2
    text-[var(--c-80)]
    bg-[var(--c-20)]
    cursor-pointer
    text-center
    drop-shadow-md
  ">
    <span class="flex justify-center items-center text-sm">
      <span id="scrollpercent"><span>0</span> %</span>
      <iconify-icon width="18" icon="mingcute:arrow-to-up-fill" id="go-top"></iconify-icon>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
